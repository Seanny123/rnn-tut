{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tutorial is taken from http://peterroelants.github.io/posts/rnn_implementation_part01/\n",
    "# Then I renamed the variables to be a bit more intuitive\n",
    "\n",
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "from matplotlib import cm  # Colormaps\n",
    "from matplotlib.colors import LogNorm  # Log colormaps\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "nb_of_samples = 20\n",
    "sequence_len = 10\n",
    "# Create the sequences\n",
    "X = np.zeros((nb_of_samples, sequence_len))\n",
    "for row_idx in range(nb_of_samples):\n",
    "    X[row_idx,:] = np.around(np.random.rand(sequence_len)).astype(int)\n",
    "# Create the targets for each sequence\n",
    "t = np.sum(X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the forward step functions\n",
    "def update_state(current_input, previous_state, wx, wRec):\n",
    "    \"\"\"\n",
    "    Compute state k from the previous state (sk) and current input (xk),\n",
    "    by use of the input weights (wx) and recursive weights (wRec).\n",
    "    \"\"\"\n",
    "    return current_input * wx + previous_state * wRec\n",
    "    \n",
    "def forward_states(input_X, wx, wRec):\n",
    "    \"\"\"\n",
    "    Unfold the network and compute all state activations given the input X,\n",
    "    and input weights (wx) and recursive weights (wRec).\n",
    "    Return the state activations in a matrix, the last column states[:,-1] contains the\n",
    "    final activations.\n",
    "    \"\"\"\n",
    "    states = np.zeros((input_X.shape[0], input_X.shape[1]+1))\n",
    "    for in_x in range(0, input_X.shape[1]):\n",
    "        states[:, in_x+1] = update_state(input_X[:, in_x], states[:, in_x], wx, wRec)\n",
    "    return states\n",
    "    \n",
    "def cost(y, t): \n",
    "    \"\"\"\n",
    "    Return the MSE between the targets t and the outputs y.\n",
    "    \"\"\"\n",
    "    return ((t - y)**2).sum() / nb_of_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# backward step functions\n",
    "\n",
    "def output_gradient(y, t):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the MSE cost function with respect to the output y.\n",
    "    \"\"\"\n",
    "    return 2.0 * (y - t) / nb_of_samples\n",
    "\n",
    "def backward_gradient(input_X, states, grad_out, wRec):\n",
    "    \"\"\"\n",
    "    Backpropagate the gradient computed at the output (grad_out) through the network.\n",
    "    Accumulate the parameter gradients for wX and wRec by for each layer by addition.\n",
    "    Return the parameter gradients as a tuple, and the gradients at the output of each layer.\n",
    "    \"\"\"\n",
    "    grad_over_time = np.zeros((input_X.shape[0], input_X.shape[1]+1))\n",
    "    grad_over_time[:, -1] = grad_out\n",
    "    wx_grad = 0\n",
    "    wRec_grad = 0\n",
    "    # go backwards\n",
    "    for k in range(X.shape[1], 0, -1):\n",
    "        # scale the gradient at each level by the previous input\n",
    "        wx_grad += np.sum(grad_over_time[:, k] * input_X[:, k-1])\n",
    "        # scale the gradient at each level by the previous state\n",
    "        wRec_grad += np.sum(grad_over_time[:, k] * states[:, k-1])\n",
    "        # only the recurrent connection carries over through time\n",
    "        grad_over_time[:, k-1] = grad_over_time[:, k] * wRec\n",
    "    return (wx_grad, wRec_grad), grad_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Rprop optimisation function to minimize exploding gradients\n",
    "def update_rprop(X, t, W, W_prev_sign, W_delta, eta_p, eta_n):\n",
    "    \"\"\"\n",
    "    Update Rprop values in one iteration.\n",
    "    X: input data.\n",
    "    t: targets.\n",
    "    W: Current weight parameters.\n",
    "    W_prev_sign: Previous sign of the W gradient.\n",
    "    W_delta: Rprop update values (Delta).\n",
    "    eta_p, eta_n: Rprop hyperparameters.\n",
    "    \"\"\"\n",
    "    # Perform forward and backward pass to get the gradients\n",
    "    S = forward_states(X, W[0], W[1])\n",
    "    grad_out = output_gradient(S[:,-1], t)\n",
    "    W_grads, _ = backward_gradient(X, S, grad_out, W[1])\n",
    "    W_sign = np.sign(W_grads)  # Sign of new gradient\n",
    "    # Update the Delta (update value) for each weight parameter seperately\n",
    "    for i, _ in enumerate(W):\n",
    "        if W_sign[i] == W_prev_sign[i]:\n",
    "            W_delta[i] *= eta_p\n",
    "        else:\n",
    "            W_delta[i] *= eta_n\n",
    "    return W_delta, W_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights are: wx = 1.00135554721,  wRec = 0.999674473785\n"
     ]
    }
   ],
   "source": [
    "# Perform Rprop optimisation\n",
    "\n",
    "# Set hyperparameters\n",
    "eta_p = 1.2\n",
    "eta_n = 0.5\n",
    "\n",
    "# Set initial parameters\n",
    "W = [-1.5, 2]  # [wx, wRec]\n",
    "W_delta = [0.001, 0.001]  # Update values (Delta) for W\n",
    "W_sign = [0, 0]  # Previous sign of W\n",
    "\n",
    "# Iterate over 500 iterations\n",
    "for i in range(500):\n",
    "    # Get the update values and sign of the last gradient\n",
    "    W_delta, W_sign = update_rprop(X, t, W, W_sign, W_delta, eta_p, eta_n)\n",
    "    # Update each weight parameter seperately\n",
    "    for i, _ in enumerate(W):\n",
    "        W[i] -= W_sign[i] * W_delta[i]\n",
    "\n",
    "print('Final weights are: wx = %s,  wRec = %s' %(W[0], W[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target output: 5 vs Model output: 5.00\n"
     ]
    }
   ],
   "source": [
    "test_inpt = np.asmatrix([[0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1]])\n",
    "test_outpt = forward_states(test_inpt, W[0], W[1])[:,-1]\n",
    "print 'Target output: {:d} vs Model output: {:.2f}'.format(test_inpt.sum(), test_outpt[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
